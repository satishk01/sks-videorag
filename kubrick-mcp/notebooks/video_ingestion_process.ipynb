{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixeltable Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore the video processing pipeline we've implemented using Pixeltable. The idea is to get an intuition of what Pixeltable does and also understand what the MCP server will be doing under the hood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to add your own videos for this notebook, please make sure you have converted them using this command:\n",
    "\n",
    "`make convert-video input=<input_video_path> output=<output_video_path>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_path = \"./data/pass_the_butter_rick_and_morty.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the video table and inserting the video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pixeltable as pxt\n",
    "\n",
    "pxt.drop_dir(\"test\", force=True)\n",
    "pxt.create_dir(\"test\")\n",
    "\n",
    "video_table = pxt.create_table(\n",
    "    \"test.videos\", schema={\"video\": pxt.Video}, if_exists=\"replace_force\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_table.insert([{\"video\": video_path}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you recognise this Rick & Morty episode?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extracting the audio from the video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pixeltable.functions.video import extract_audio\n",
    "\n",
    "video_table.add_computed_column(\n",
    "    audio=extract_audio(video_table.video, format=\"mp3\"), if_exists=\"ignore\"\n",
    ")\n",
    "video_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the table above, we have two columns now. One column contains the video, and the other column contains the audio. Now, we need to split the audio into chunks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Splitting the audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pixeltable.iterators.audio import AudioSplitter\n",
    "\n",
    "audio_view = pxt.create_view(\n",
    "    \"test.audio_chunks\",\n",
    "    video_table,\n",
    "    iterator=AudioSplitter.create(\n",
    "        audio=video_table.audio,\n",
    "        chunk_duration_sec=10.0,\n",
    "        overlap_sec=1.0,\n",
    "        min_chunk_duration_sec=1.0,\n",
    "    ),\n",
    "    if_exists=\"replace_force\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_view.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, apart from the `audio_chunk` column, we also have two additional columns, giving information about the start and end time in seconds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Transcription with GPT-4o Mini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pixeltable.functions import openai\n",
    "\n",
    "audio_view.add_computed_column(\n",
    "    transcription=openai.transcriptions(\n",
    "        audio=audio_view.audio_chunk, model=\"gpt-4o-mini-transcribe\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_view.order_by(audio_view.pos).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new column has appeared, containing the transcription about the corresponding audio chunk! If you look at the content, you'll notice it has a lot of metadata information. We just need the `text` attribute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Extracting the text from the transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kubrick_mcp.video.ingestion.functions import extract_text_from_chunk\n",
    "\n",
    "audio_view.add_computed_column(\n",
    "    chunk_text=extract_text_from_chunk(audio_view.transcription),\n",
    "    if_exists=\"ignore\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_view.order_by(audio_view.pos).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the `chunk_text` column, you'll see the text transcription (just the string).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Creating an embedding index for audio transcriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pixeltable.functions.openai import embeddings\n",
    "\n",
    "embed_model = embeddings.using(model=\"text-embedding-3-small\")\n",
    "\n",
    "audio_view.add_embedding_index(\n",
    "    column=audio_view.chunk_text,\n",
    "    string_embed=embed_model,\n",
    "    if_exists=\"replace_force\",\n",
    "    idx_name=\"chunks_index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check how this embedding index works by passing some query. The code below will take care of check the most similar audio_chunk to the user query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_sims = audio_view.chunk_text.similarity(\"you pass the butter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = audio_view.select(\n",
    "    audio_view.pos,\n",
    "    audio_view.start_time_sec,\n",
    "    audio_view.end_time_sec,\n",
    "    similarity=audio_sims,\n",
    ").order_by(audio_sims, asc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k_entry = results.limit(1).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the clip by using the `extract_video_clip` helper function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kubrick_mcp.video.ingestion.tools import extract_video_clip\n",
    "\n",
    "video_clip = extract_video_clip(\n",
    "    video_path=video_path,\n",
    "    start_time=top_k_entry[\"start_time_sec\"],\n",
    "    end_time=top_k_entry[\"end_time_sec\"],\n",
    "    output_path=\"./data/pass_the_butter_clip_sim_audio.mp4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(video_clip.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Splitting the video in frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pixeltable.iterators.video import FrameIterator\n",
    "\n",
    "frames_view = pxt.create_view(\n",
    "    \"test.frames_view\",\n",
    "    video_table,\n",
    "    iterator=FrameIterator.create(video=video_table.video, fps=0.5),\n",
    "    if_exists=\"ignore\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames_view.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access visual content, we need to sample the video, placing each frame in a row. After that, we just need to create an embedding index for the frames. We'll use CLIP embeddings for this one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Creating frame embedding index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pixeltable.functions.huggingface import clip\n",
    "\n",
    "frames_view.add_embedding_index(\n",
    "    column=frames_view.frame,\n",
    "    image_embed=clip.using(model_id=\"openai/clip-vit-base-patch32\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with the audio embedding index, we can also provide an image and check the clip more similar to it. For example, let's use the following picture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"./data/sad_robot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_sims = frames_view.frame.similarity(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = frames_view.select(\n",
    "    frames_view.pos_msec,\n",
    "    frames_view.frame,\n",
    "    similarity=image_sims,\n",
    ").order_by(image_sims, asc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k_entry = results.limit(1).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k_entry[\"frame\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kubrick_mcp.video.ingestion.tools import extract_video_clip\n",
    "\n",
    "\n",
    "video_clip = extract_video_clip(\n",
    "    video_path=video_path,\n",
    "    start_time=top_k_entry[\"pos_msec\"] / 1000.0 - 3,\n",
    "    end_time=top_k_entry[\"pos_msec\"] / 1000.0 + 3,\n",
    "    output_path=\"./data/pass_the_butter_clip_sim_image.mp4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(video_clip.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Add captions for each frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.openai import vision\n",
    "\n",
    "frames_view.add_computed_column(\n",
    "    im_caption=vision(\n",
    "        prompt=\"Describe what is happening in the image\",\n",
    "        image=frames_view.frame,\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Creating caption embedding index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_model = embeddings.using(model=\"text-embedding-3-small\")\n",
    "\n",
    "frames_view.add_embedding_index(column=frames_view.im_caption, string_embed=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "caption_sims = frames_view.im_caption.similarity(\n",
    "    \"Text message saying 'your father is insecure about his intelligence'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "caption_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = frames_view.select(\n",
    "    frames_view.pos_msec,\n",
    "    frames_view.im_caption,\n",
    "    similarity=caption_sims,\n",
    ").order_by(caption_sims, asc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k_entry = results.limit(1).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kubrick_mcp.video.ingestion.tools import extract_video_clip\n",
    "\n",
    "video_clip = extract_video_clip(\n",
    "    video_path=video_path,\n",
    "    start_time=top_k_entry[\"pos_msec\"] / 1000.0 - 3,\n",
    "    end_time=top_k_entry[\"pos_msec\"] / 1000.0 + 3,\n",
    "    output_path=\"./data/pass_the_butter_clip_sim_caption2.mp4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"./data/pass_the_butter_clip_sim_caption.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three embedding indexes will be used used by our MCP Server to fetch the most relevant content from the videos. But of course, the Agent won't be aware of the inner workings, since that's exactly the goal of the MCP Server!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
